<div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n           <div itemprop=\"articleBody\">\n            \n  <div class=\"section\" id=\"command-line-tool\">\n<span id=\"topics-commands\"></span><h1>Command line tool<a class=\"headerlink\" href=\"#command-line-tool\" title=\"Permalink to this headline\">¶</a></h1>\n<div class=\"versionadded\">\n<p><span class=\"versionmodified\">New in version 0.10.</span></p>\n</div>\n<p>Scrapy is controlled through the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command-line tool, to be referred\nhere as the “Scrapy tool” to differentiate it from the sub-commands, which we\njust call “commands” or “Scrapy commands”.</p>\n<p>The Scrapy tool provides several commands, for multiple purposes, and each one\naccepts a different set of arguments and options.</p>\n<p>(The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">deploy</span></code> command has been removed in 1.0 in favor of the\nstandalone <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapyd-deploy</span></code>. See <a class=\"reference external\" href=\"https://scrapyd.readthedocs.io/en/latest/deploy.html\">Deploying your project</a>.)</p>\n<div class=\"section\" id=\"configuration-settings\">\n<span id=\"topics-config-settings\"></span><h2>Configuration settings<a class=\"headerlink\" href=\"#configuration-settings\" title=\"Permalink to this headline\">¶</a></h2>\n<p>Scrapy will look for configuration parameters in ini-style <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.cfg</span></code> files\nin standard locations:</p>\n<ol class=\"arabic simple\">\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">/etc/scrapy.cfg</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">c:\\scrapy\\scrapy.cfg</span></code> (system-wide),</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">~/.config/scrapy.cfg</span></code> (<code class=\"docutils literal notranslate\"><span class=\"pre\">$XDG_CONFIG_HOME</span></code>) and <code class=\"docutils literal notranslate\"><span class=\"pre\">~/.scrapy.cfg</span></code> (<code class=\"docutils literal notranslate\"><span class=\"pre\">$HOME</span></code>)\nfor global (user-wide) settings, and</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.cfg</span></code> inside a scrapy project’s root (see next section).</li>\n</ol>\n<p>Settings from these files are merged in the listed order of preference:\nuser-defined values have higher priority than system-wide defaults\nand project-wide settings will override all others, when defined.</p>\n<p>Scrapy also understands, and can be configured through, a number of environment\nvariables. Currently these are:</p>\n<ul class=\"simple\">\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_SETTINGS_MODULE</span></code> (see <a class=\"reference internal\" href=\"settings.html#topics-settings-module-envvar\"><span class=\"std std-ref\">Designating the settings</span></a>)</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_PROJECT</span></code> (see <a class=\"reference internal\" href=\"#topics-project-envvar\"><span class=\"std std-ref\">Sharing the root directory between projects</span></a>)</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_PYTHON_SHELL</span></code> (see <a class=\"reference internal\" href=\"shell.html#topics-shell\"><span class=\"std std-ref\">Scrapy shell</span></a>)</li>\n</ul>\n</div>\n<div class=\"section\" id=\"default-structure-of-scrapy-projects\">\n<span id=\"topics-project-structure\"></span><h2>Default structure of Scrapy projects<a class=\"headerlink\" href=\"#default-structure-of-scrapy-projects\" title=\"Permalink to this headline\">¶</a></h2>\n<p>Before delving into the command-line tool and its sub-commands, let’s first\nunderstand the directory structure of a Scrapy project.</p>\n<p>Though it can be modified, all Scrapy projects have the same file\nstructure by default, similar to this:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">cfg</span>\n<span class=\"n\">myproject</span><span class=\"o\">/</span>\n    <span class=\"fm\">__init__</span><span class=\"o\">.</span><span class=\"n\">py</span>\n    <span class=\"n\">items</span><span class=\"o\">.</span><span class=\"n\">py</span>\n    <span class=\"n\">middlewares</span><span class=\"o\">.</span><span class=\"n\">py</span>\n    <span class=\"n\">pipelines</span><span class=\"o\">.</span><span class=\"n\">py</span>\n    <span class=\"n\">settings</span><span class=\"o\">.</span><span class=\"n\">py</span>\n    <span class=\"n\">spiders</span><span class=\"o\">/</span>\n        <span class=\"fm\">__init__</span><span class=\"o\">.</span><span class=\"n\">py</span>\n        <span class=\"n\">spider1</span><span class=\"o\">.</span><span class=\"n\">py</span>\n        <span class=\"n\">spider2</span><span class=\"o\">.</span><span class=\"n\">py</span>\n        <span class=\"o\">...</span>\n</pre></div>\n</div>\n<p>The directory where the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.cfg</span></code> file resides is known as the <em>project\nroot directory</em>. That file contains the name of the python module that defines\nthe project settings. Here is an example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"p\">[</span><span class=\"n\">settings</span><span class=\"p\">]</span>\n<span class=\"n\">default</span> <span class=\"o\">=</span> <span class=\"n\">myproject</span><span class=\"o\">.</span><span class=\"n\">settings</span>\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"sharing-the-root-directory-between-projects\">\n<span id=\"topics-project-envvar\"></span><h2>Sharing the root directory between projects<a class=\"headerlink\" href=\"#sharing-the-root-directory-between-projects\" title=\"Permalink to this headline\">¶</a></h2>\n<p>A project root directory, the one that contains the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.cfg</span></code>, may be\nshared by multiple Scrapy projects, each with its own settings module.</p>\n<p>In that case, you must define one or more aliases for those settings modules\nunder <code class=\"docutils literal notranslate\"><span class=\"pre\">[settings]</span></code> in your <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.cfg</span></code> file:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"p\">[</span><span class=\"n\">settings</span><span class=\"p\">]</span>\n<span class=\"n\">default</span> <span class=\"o\">=</span> <span class=\"n\">myproject1</span><span class=\"o\">.</span><span class=\"n\">settings</span>\n<span class=\"n\">project1</span> <span class=\"o\">=</span> <span class=\"n\">myproject1</span><span class=\"o\">.</span><span class=\"n\">settings</span>\n<span class=\"n\">project2</span> <span class=\"o\">=</span> <span class=\"n\">myproject2</span><span class=\"o\">.</span><span class=\"n\">settings</span>\n</pre></div>\n</div>\n<p>By default, the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command-line tool will use the <code class=\"docutils literal notranslate\"><span class=\"pre\">default</span></code> settings.\nUse the <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_PROJECT</span></code> environment variable to specify a different project\nfor <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> to use:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy settings --get BOT_NAME\nProject 1 Bot\n$ export SCRAPY_PROJECT=project2\n$ scrapy settings --get BOT_NAME\nProject 2 Bot\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"using-the-scrapy-tool\">\n<h2>Using the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> tool<a class=\"headerlink\" href=\"#using-the-scrapy-tool\" title=\"Permalink to this headline\">¶</a></h2>\n<p>You can start by running the Scrapy tool with no arguments and it will print\nsome usage help and the available commands:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">Scrapy</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">Y</span> <span class=\"o\">-</span> <span class=\"n\">no</span> <span class=\"n\">active</span> <span class=\"n\">project</span>\n\n<span class=\"n\">Usage</span><span class=\"p\">:</span>\n  <span class=\"n\">scrapy</span> <span class=\"o\">&lt;</span><span class=\"n\">command</span><span class=\"o\">&gt;</span> <span class=\"p\">[</span><span class=\"n\">options</span><span class=\"p\">]</span> <span class=\"p\">[</span><span class=\"n\">args</span><span class=\"p\">]</span>\n\n<span class=\"n\">Available</span> <span class=\"n\">commands</span><span class=\"p\">:</span>\n  <span class=\"n\">crawl</span>         <span class=\"n\">Run</span> <span class=\"n\">a</span> <span class=\"n\">spider</span>\n  <span class=\"n\">fetch</span>         <span class=\"n\">Fetch</span> <span class=\"n\">a</span> <span class=\"n\">URL</span> <span class=\"n\">using</span> <span class=\"n\">the</span> <span class=\"n\">Scrapy</span> <span class=\"n\">downloader</span>\n<span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">]</span>\n</pre></div>\n</div>\n<p>The first line will print the currently active project if you’re inside a\nScrapy project. In this example it was run from outside a project. If run from inside\na project it would have printed something like this:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">Scrapy</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">Y</span> <span class=\"o\">-</span> <span class=\"n\">project</span><span class=\"p\">:</span> <span class=\"n\">myproject</span>\n\n<span class=\"n\">Usage</span><span class=\"p\">:</span>\n  <span class=\"n\">scrapy</span> <span class=\"o\">&lt;</span><span class=\"n\">command</span><span class=\"o\">&gt;</span> <span class=\"p\">[</span><span class=\"n\">options</span><span class=\"p\">]</span> <span class=\"p\">[</span><span class=\"n\">args</span><span class=\"p\">]</span>\n\n<span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">]</span>\n</pre></div>\n</div>\n<div class=\"section\" id=\"creating-projects\">\n<h3>Creating projects<a class=\"headerlink\" href=\"#creating-projects\" title=\"Permalink to this headline\">¶</a></h3>\n<p>The first thing you typically do with the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> tool is create your Scrapy\nproject:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">scrapy</span> <span class=\"n\">startproject</span> <span class=\"n\">myproject</span> <span class=\"p\">[</span><span class=\"n\">project_dir</span><span class=\"p\">]</span>\n</pre></div>\n</div>\n<p>That will create a Scrapy project under the <code class=\"docutils literal notranslate\"><span class=\"pre\">project_dir</span></code> directory.\nIf <code class=\"docutils literal notranslate\"><span class=\"pre\">project_dir</span></code> wasn’t specified, <code class=\"docutils literal notranslate\"><span class=\"pre\">project_dir</span></code> will be the same as <code class=\"docutils literal notranslate\"><span class=\"pre\">myproject</span></code>.</p>\n<p>Next, you go inside the new project directory:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">cd</span> <span class=\"n\">project_dir</span>\n</pre></div>\n</div>\n<p>And you’re ready to use the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command to manage and control your\nproject from there.</p>\n</div>\n<div class=\"section\" id=\"controlling-projects\">\n<h3>Controlling projects<a class=\"headerlink\" href=\"#controlling-projects\" title=\"Permalink to this headline\">¶</a></h3>\n<p>You use the <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> tool from inside your projects to control and manage\nthem.</p>\n<p>For example, to create a new spider:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">scrapy</span> <span class=\"n\">genspider</span> <span class=\"n\">mydomain</span> <span class=\"n\">mydomain</span><span class=\"o\">.</span><span class=\"n\">com</span>\n</pre></div>\n</div>\n<p>Some Scrapy commands (like <a class=\"reference internal\" href=\"#std:command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a>) must be run from inside a Scrapy\nproject. See the <a class=\"reference internal\" href=\"#topics-commands-ref\"><span class=\"std std-ref\">commands reference</span></a> below for more\ninformation on which commands must be run from inside projects, and which not.</p>\n<p>Also keep in mind that some commands may have slightly different behaviours\nwhen running them from inside projects. For example, the fetch command will use\nspider-overridden behaviours (such as the <code class=\"docutils literal notranslate\"><span class=\"pre\">user_agent</span></code> attribute to override\nthe user-agent) if the url being fetched is associated with some specific\nspider. This is intentional, as the <code class=\"docutils literal notranslate\"><span class=\"pre\">fetch</span></code> command is meant to be used to\ncheck how spiders are downloading pages.</p>\n</div>\n</div>\n<div class=\"section\" id=\"available-tool-commands\">\n<span id=\"topics-commands-ref\"></span><h2>Available tool commands<a class=\"headerlink\" href=\"#available-tool-commands\" title=\"Permalink to this headline\">¶</a></h2>\n<p>This section contains a list of the available built-in commands with a\ndescription and some usage examples. Remember, you can always get more info\nabout each command by running:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">scrapy</span> <span class=\"o\">&lt;</span><span class=\"n\">command</span><span class=\"o\">&gt;</span> <span class=\"o\">-</span><span class=\"n\">h</span>\n</pre></div>\n</div>\n<p>And you can see all available commands with:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">scrapy</span> <span class=\"o\">-</span><span class=\"n\">h</span>\n</pre></div>\n</div>\n<p>There are two kinds of commands, those that only work from inside a Scrapy\nproject (Project-specific commands) and those that also work without an active\nScrapy project (Global commands), though they may behave slightly different\nwhen running from inside a project (as they would use the project overridden\nsettings).</p>\n<p>Global commands:</p>\n<ul class=\"simple\">\n<li><a class=\"reference internal\" href=\"#std:command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-genspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">genspider</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-settings\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">settings</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-shell\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">shell</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-fetch\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">fetch</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-view\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">view</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-version\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">version</span></code></a></li>\n</ul>\n<p>Project-only commands:</p>\n<ul class=\"simple\">\n<li><a class=\"reference internal\" href=\"#std:command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-check\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">check</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-list\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">list</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-edit\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">edit</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-parse\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">parse</span></code></a></li>\n<li><a class=\"reference internal\" href=\"#std:command-bench\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">bench</span></code></a></li>\n</ul>\n<div class=\"section\" id=\"startproject\">\n<span id=\"std:command-startproject\"></span><h3>startproject<a class=\"headerlink\" href=\"#startproject\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">startproject</span> <span class=\"pre\">&lt;project_name&gt;</span> <span class=\"pre\">[project_dir]</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Creates a new Scrapy project named <code class=\"docutils literal notranslate\"><span class=\"pre\">project_name</span></code>, under the <code class=\"docutils literal notranslate\"><span class=\"pre\">project_dir</span></code>\ndirectory.\nIf <code class=\"docutils literal notranslate\"><span class=\"pre\">project_dir</span></code> wasn’t specified, <code class=\"docutils literal notranslate\"><span class=\"pre\">project_dir</span></code> will be the same as <code class=\"docutils literal notranslate\"><span class=\"pre\">project_name</span></code>.</p>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy startproject myproject\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"genspider\">\n<span id=\"std:command-genspider\"></span><h3>genspider<a class=\"headerlink\" href=\"#genspider\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">genspider</span> <span class=\"pre\">[-t</span> <span class=\"pre\">template]</span> <span class=\"pre\">&lt;name&gt;</span> <span class=\"pre\">&lt;domain&gt;</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Create a new spider in the current folder or in the current project’s <code class=\"docutils literal notranslate\"><span class=\"pre\">spiders</span></code> folder, if called from inside a project. The <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;name&gt;</span></code> parameter is set as the spider’s <code class=\"docutils literal notranslate\"><span class=\"pre\">name</span></code>, while <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;domain&gt;</span></code> is used to generate the <code class=\"docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">start_urls</span></code> spider’s attributes.</p>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy genspider -l\nAvailable templates:\n  basic\n  crawl\n  csvfeed\n  xmlfeed\n\n$ scrapy genspider example example.com\nCreated spider 'example' using template 'basic'\n\n$ scrapy genspider -t crawl scrapyorg scrapy.org\nCreated spider 'scrapyorg' using template 'crawl'\n</pre></div>\n</div>\n<p>This is just a convenience shortcut command for creating spiders based on\npre-defined templates, but certainly not the only way to create spiders. You\ncan just create the spider source code files yourself, instead of using this\ncommand.</p>\n</div>\n<div class=\"section\" id=\"crawl\">\n<span id=\"std:command-crawl\"></span><h3>crawl<a class=\"headerlink\" href=\"#crawl\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">crawl</span> <span class=\"pre\">&lt;spider&gt;</span></code></li>\n<li>Requires project: <em>yes</em></li>\n</ul>\n<p>Start crawling using a spider.</p>\n<p>Usage examples:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy crawl myspider\n[ ... myspider starts crawling ... ]\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"check\">\n<span id=\"std:command-check\"></span><h3>check<a class=\"headerlink\" href=\"#check\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">check</span> <span class=\"pre\">[-l]</span> <span class=\"pre\">&lt;spider&gt;</span></code></li>\n<li>Requires project: <em>yes</em></li>\n</ul>\n<p>Run contract checks.</p>\n<p>Usage examples:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy check -l\nfirst_spider\n  * parse\n  * parse_item\nsecond_spider\n  * parse\n  * parse_item\n\n$ scrapy check\n[FAILED] first_spider:parse_item\n&gt;&gt;&gt; 'RetailPricex' field is missing\n\n[FAILED] first_spider:parse\n&gt;&gt;&gt; Returned 92 requests, expected 0..4\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"list\">\n<span id=\"std:command-list\"></span><h3>list<a class=\"headerlink\" href=\"#list\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">list</span></code></li>\n<li>Requires project: <em>yes</em></li>\n</ul>\n<p>List all available spiders in the current project. The output is one spider per\nline.</p>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy list\nspider1\nspider2\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"edit\">\n<span id=\"std:command-edit\"></span><h3>edit<a class=\"headerlink\" href=\"#edit\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">edit</span> <span class=\"pre\">&lt;spider&gt;</span></code></li>\n<li>Requires project: <em>yes</em></li>\n</ul>\n<p>Edit the given spider using the editor defined in the <code class=\"docutils literal notranslate\"><span class=\"pre\">EDITOR</span></code> environment\nvariable or (if unset) the <a class=\"reference internal\" href=\"settings.html#std:setting-EDITOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EDITOR</span></code></a> setting.</p>\n<p>This command is provided only as a convenience shortcut for the most common\ncase, the developer is of course free to choose any tool or IDE to write and\ndebug spiders.</p>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy edit spider1\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"fetch\">\n<span id=\"std:command-fetch\"></span><h3>fetch<a class=\"headerlink\" href=\"#fetch\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">fetch</span> <span class=\"pre\">&lt;url&gt;</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Downloads the given URL using the Scrapy downloader and writes the contents to\nstandard output.</p>\n<p>The interesting thing about this command is that it fetches the page how the\nspider would download it. For example, if the spider has a <code class=\"docutils literal notranslate\"><span class=\"pre\">USER_AGENT</span></code>\nattribute which overrides the User Agent, it will use that one.</p>\n<p>So this command can be used to “see” how your spider would fetch a certain page.</p>\n<p>If used outside a project, no particular per-spider behaviour would be applied\nand it will just use the default Scrapy downloader settings.</p>\n<p>Supported options:</p>\n<ul class=\"simple\">\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--headers</span></code>: print the response’s HTTP headers instead of the response’s body</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them)</li>\n</ul>\n<p>Usage examples:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy fetch --nolog http://www.example.com/some/page.html\n[ ... html content here ... ]\n\n$ scrapy fetch --nolog --headers http://www.example.com/\n{'Accept-Ranges': ['bytes'],\n 'Age': ['1263   '],\n 'Connection': ['close     '],\n 'Content-Length': ['596'],\n 'Content-Type': ['text/html; charset=UTF-8'],\n 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],\n 'Etag': ['\"573c1-254-48c9c87349680\"'],\n 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],\n 'Server': ['Apache/2.2.3 (CentOS)']}\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"view\">\n<span id=\"std:command-view\"></span><h3>view<a class=\"headerlink\" href=\"#view\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">view</span> <span class=\"pre\">&lt;url&gt;</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Opens the given URL in a browser, as your Scrapy spider would “see” it.\nSometimes spiders see pages differently from regular users, so this can be used\nto check what the spider “sees” and confirm it’s what you expect.</p>\n<p>Supported options:</p>\n<ul class=\"simple\">\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them)</li>\n</ul>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy view http://www.example.com/some/page.html\n[ ... browser starts ... ]\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"shell\">\n<span id=\"std:command-shell\"></span><h3>shell<a class=\"headerlink\" href=\"#shell\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span> <span class=\"pre\">[url]</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL is\ngiven. Also supports UNIX-style local file paths, either relative with\n<code class=\"docutils literal notranslate\"><span class=\"pre\">./</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">../</span></code> prefixes or absolute file paths.\nSee <a class=\"reference internal\" href=\"shell.html#topics-shell\"><span class=\"std std-ref\">Scrapy shell</span></a> for more info.</p>\n<p>Supported options:</p>\n<ul class=\"simple\">\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">-c</span> <span class=\"pre\">code</span></code>: evaluate the code in the shell, print the result and exit</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them);\nthis only affects the URL you may pass as argument on the command line;\nonce you are inside the shell, <code class=\"docutils literal notranslate\"><span class=\"pre\">fetch(url)</span></code> will still follow HTTP redirects by default.</li>\n</ul>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy shell http://www.example.com/some/page.html\n[ ... scrapy shell starts ... ]\n\n$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'\n(200, 'http://www.example.com/')\n\n# shell follows HTTP redirects by default\n$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n(200, 'http://example.com/')\n\n# you can disable this with --no-redirect\n# (only for the URL passed as command line argument)\n$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"parse\">\n<span id=\"std:command-parse\"></span><h3>parse<a class=\"headerlink\" href=\"#parse\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">parse</span> <span class=\"pre\">&lt;url&gt;</span> <span class=\"pre\">[options]</span></code></li>\n<li>Requires project: <em>yes</em></li>\n</ul>\n<p>Fetches the given URL and parses it with the spider that handles it, using the\nmethod passed with the <code class=\"docutils literal notranslate\"><span class=\"pre\">--callback</span></code> option, or <code class=\"docutils literal notranslate\"><span class=\"pre\">parse</span></code> if not given.</p>\n<p>Supported options:</p>\n<ul class=\"simple\">\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--a</span> <span class=\"pre\">NAME=VALUE</span></code>: set spider argument (may be repeated)</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--callback</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">-c</span></code>: spider method to use as callback for parsing the\nresponse</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--meta</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">-m</span></code>: additional request meta that will be passed to the callback\nrequest. This must be a valid json string. Example: –meta=’{“foo” : “bar”}’</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--cbkwargs</span></code>: additional keyword arguments that will be passed to the callback.\nThis must be a valid json string. Example: –cbkwargs=’{“foo” : “bar”}’</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--pipelines</span></code>: process items through pipelines</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--rules</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">-r</span></code>: use <a class=\"reference internal\" href=\"spiders.html#scrapy.spiders.CrawlSpider\" title=\"scrapy.spiders.CrawlSpider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlSpider</span></code></a>\nrules to discover the callback (i.e. spider method) to use for parsing the\nresponse</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--noitems</span></code>: don’t show scraped items</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--nolinks</span></code>: don’t show extracted links</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--nocolour</span></code>: avoid using pygments to colorize the output</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--depth</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">-d</span></code>: depth level for which the requests should be followed\nrecursively (default: 1)</li>\n<li><code class=\"docutils literal notranslate\"><span class=\"pre\">--verbose</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">-v</span></code>: display information for each depth level</li>\n</ul>\n<p>Usage example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy parse http://www.example.com/ -c parse_item\n[ ... scrapy log lines crawling example.com spider ... ]\n\n&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;\n# Scraped Items  ------------------------------------------------------------\n[{'name': 'Example item',\n 'category': 'Furniture',\n 'length': '12 cm'}]\n\n# Requests  -----------------------------------------------------------------\n[]\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"settings\">\n<span id=\"std:command-settings\"></span><h3>settings<a class=\"headerlink\" href=\"#settings\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">settings</span> <span class=\"pre\">[options]</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Get the value of a Scrapy setting.</p>\n<p>If used inside a project it’ll show the project setting value, otherwise it’ll\nshow the default Scrapy value for that setting.</p>\n<p>Example usage:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy settings --get BOT_NAME\nscrapybot\n$ scrapy settings --get DOWNLOAD_DELAY\n0\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"runspider\">\n<span id=\"std:command-runspider\"></span><h3>runspider<a class=\"headerlink\" href=\"#runspider\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">runspider</span> <span class=\"pre\">&lt;spider_file.py&gt;</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Run a spider self-contained in a Python file, without having to create a\nproject.</p>\n<p>Example usage:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span>$ scrapy runspider myspider.py\n[ ... spider starts crawling ... ]\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"version\">\n<span id=\"std:command-version\"></span><h3>version<a class=\"headerlink\" href=\"#version\" title=\"Permalink to this headline\">¶</a></h3>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">version</span> <span class=\"pre\">[-v]</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Prints the Scrapy version. If used with <code class=\"docutils literal notranslate\"><span class=\"pre\">-v</span></code> it also prints Python, Twisted\nand Platform info, which is useful for bug reports.</p>\n</div>\n<div class=\"section\" id=\"bench\">\n<span id=\"std:command-bench\"></span><h3>bench<a class=\"headerlink\" href=\"#bench\" title=\"Permalink to this headline\">¶</a></h3>\n<div class=\"versionadded\">\n<p><span class=\"versionmodified\">New in version 0.17.</span></p>\n</div>\n<ul class=\"simple\">\n<li>Syntax: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">bench</span></code></li>\n<li>Requires project: <em>no</em></li>\n</ul>\n<p>Run a quick benchmark test. <a class=\"reference internal\" href=\"benchmarking.html#benchmarking\"><span class=\"std std-ref\">Benchmarking</span></a>.</p>\n</div>\n</div>\n<div class=\"section\" id=\"custom-project-commands\">\n<h2>Custom project commands<a class=\"headerlink\" href=\"#custom-project-commands\" title=\"Permalink to this headline\">¶</a></h2>\n<p>You can also add your custom project commands by using the\n<a class=\"reference internal\" href=\"#std:setting-COMMANDS_MODULE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">COMMANDS_MODULE</span></code></a> setting. See the Scrapy commands in\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/tree/master/scrapy/commands\">scrapy/commands</a> for examples on how to implement your commands.</p>\n<div class=\"section\" id=\"commands-module\">\n<span id=\"std:setting-COMMANDS_MODULE\"></span><h3>COMMANDS_MODULE<a class=\"headerlink\" href=\"#commands-module\" title=\"Permalink to this headline\">¶</a></h3>\n<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">''</span></code> (empty string)</p>\n<p>A module to use for looking up custom Scrapy commands. This is used to add custom\ncommands for your Scrapy project.</p>\n<p>Example:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">COMMANDS_MODULE</span> <span class=\"o\">=</span> <span class=\"s1\">'mybot.commands'</span>\n</pre></div>\n</div>\n</div>\n<div class=\"section\" id=\"register-commands-via-setup-py-entry-points\">\n<h3>Register commands via setup.py entry points<a class=\"headerlink\" href=\"#register-commands-via-setup-py-entry-points\" title=\"Permalink to this headline\">¶</a></h3>\n<div class=\"admonition note\">\n<p class=\"first admonition-title\">Note</p>\n<p class=\"last\">This is an experimental feature, use with caution.</p>\n</div>\n<p>You can also add Scrapy commands from an external library by adding a\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.commands</span></code> section in the entry points of the library <code class=\"docutils literal notranslate\"><span class=\"pre\">setup.py</span></code>\nfile.</p>\n<p>The following example adds <code class=\"docutils literal notranslate\"><span class=\"pre\">my_command</span></code> command:</p>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">setuptools</span> <span class=\"k\">import</span> <span class=\"n\">setup</span><span class=\"p\">,</span> <span class=\"n\">find_packages</span>\n\n<span class=\"n\">setup</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'scrapy-mymodule'</span><span class=\"p\">,</span>\n  <span class=\"n\">entry_points</span><span class=\"o\">=</span><span class=\"p\">{</span>\n    <span class=\"s1\">'scrapy.commands'</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n      <span class=\"s1\">'my_command=my_scrapy_module.commands:MyCommand'</span><span class=\"p\">,</span>\n    <span class=\"p\">],</span>\n  <span class=\"p\">},</span>\n <span class=\"p\">)</span>\n</pre></div>\n</div>\n</div>\n</div>\n</div>\n\n\n           </div>\n           \n          </div>
